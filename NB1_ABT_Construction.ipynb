{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Base Table Construction\n",
    "---\n",
    "Begin to explore the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM). \n",
    "\n",
    "\n",
    "Below you will find a number of steps that you will be required to complete before you can start.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "---\n",
    "\n",
    "This assignment will only be using [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB), but we will be using more than one by the end of the semster. In later steps, you will need to access the uncompressed files from these partitions, so remember where you put them.\n",
    "\n",
    "A paper describing the construction of the dataset can be found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "---\n",
    "\n",
    "Individual partitions of the dataset can be accessed through following links:\n",
    "- [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) - 1.2GB\n",
    "- [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) - 1.4GB\n",
    "- [Partition 3](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/PTPGQT) - 702.1 MB\n",
    "- [Partition 4](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/FIFLFU) - 844.4 MB\n",
    "- [Partition 5](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/QC2C3X) - 1.2 GB\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Attributes:\n",
    "\n",
    "Each file in the dataset contains the following attributes as a single variate of the multivariate timeseries (MVTS) sample. \n",
    "\n",
    "|              |                  |             |\n",
    "|--------------|------------------|-------------|\n",
    "| 1. Timestamp | 2. TOTUSJH       | 3. TOTBSQ   |\t\n",
    "| 4. TOTPOT\t   | 5. TOTUSJZ       | 6. ABSNJZH  |\t\n",
    "| 7. SAVNCPP   | 8. USFLUX        | 9. TOTFZ\t|\n",
    "| 10. MEANPOT  | 11. EPSZ\t      | 12. MEANSHR |\n",
    "| 13. SHRGT45  | 14. MEANGAM      | 15. MEANGBT |\n",
    "| 16. MEANGBZ  | 17. MEANGBH      | 18. MEANJZH |\n",
    "| 19. TOTFY    | 20. MEANJZD      | 21. MEANALP |\t\n",
    "| 22. TOTFX    | 23. EPSY\t      | 24. EPSX\t|\n",
    "| 25. R_VALUE  | 26. CRVAL1       | 27. CRLN_OBS|\t\n",
    "| 28. CRLT_OBS | 29. CRVAL2       | 30. HC_ANGLE|\t\n",
    "| 31. SPEI     | 32. LAT_MIN      | 33. LON_MIN |\n",
    "| 34. LAT_MAX  | 35. LON_MAX      | 36. QUALITY |\t\n",
    "| 37. BFLARE   | 38. BFLARE_LABEL |\t39. CFLARE  |\t\n",
    "| 39. CFLARE_LABEL | 40. MFLARE | 41. MFLARE_LABEL |\t\n",
    "| 42. XFLARE | 43. XFLARE_LABEL | 44. BFLARE_LOC |\t\n",
    "| 45. BFLARE_LABEL_LOC | 46. CFLARE_LOC | 47. CFLARE_LABEL_LOC |\t\n",
    "| 48. MFLARE_LOC | 49. MFLARE_LABEL_LOC | 50. FLARE_LOC |\t\n",
    "| 51. XFLARE_LABEL_LOC | 52. XR_MAX | 53. XR_QUAL |\t\n",
    "|54. IS_TMFI | | |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Unpacking the data\n",
    "---\n",
    "\n",
    "The partitions come in tar.gz archive files. These are easily opened on all current operating systems using the same command in the terminal.\n",
    "\n",
    "- On Windows 10: Use cmd.exe, then run: tar xf partition1_instances.tar.gz\n",
    "- On Linux: In the terminal run: tar xf partition1_instances.tar.gz\n",
    "- On Mac: In the terminal run: tar xf partition1_instances.tar.gz\n",
    "\n",
    "These all assume you are in the directory that contains the tar.gz file and that you wish to unpack in this same directory.  Search for tar commands if you wish to do something else.\n",
    "\n",
    "[Instruction Manual for Tar](https://man7.org/linux/man-pages/man1/tar.1.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "---\n",
    "\n",
    "The __partition1__ direcotry contains two subdirectories, __FL__ and __NF__, these subdirectories represent the two classes of our target feature in the solar flare prediction problem we will be attempting to solve this semester. \n",
    "\n",
    "- __FL__: Represents the multivariate time series samples that have a Solar Flare occur within 24 hours of the observation.\n",
    "- __NF__: Represents the multivariate time series samples that do not have a Solar Flare occur within 24 hours of the observation.\n",
    "\n",
    "The multivariate time series samples are stored in .csv files for each individual sample. Each file name contains a number of pieces of information that we will wish to keep for our prediction task and therefore should be part of your Analytics Base Table. Below are examples of the naming for each sample type.\n",
    "\n",
    "- __FL__ file name example:`M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv`\n",
    "- __NF__ file name example:`FQ_ar99_s2010-08-01T19:00:00_e2010-08-02T06:48:00.csv` or `B1.9@909:Primary_ar325_s2011-01-04T02:36:00_e2011-01-04T14:24:00.csv`\n",
    "\n",
    "Let's look at these formats, starting with those that contain an `@` symbol (we will use the __FL__ file as an example but note that the __NF__ data also has files with this naming):\n",
    "- __M1.0@265:Primary__: This says that there occurs an M1.0 sized flare within 24 hours of our sample. It also says that this flare is numbered 265 in the accompanying integrated flare dataset that comes as a supplementary file to this dataset. Additionally, \"Primary\" indicates that the intersection with this active region was verified through the primary method described in the paper.  \n",
    "- __\\_ar115__: This indicates which active region (`_ar`) the sample comes from in the original unsampled dataset.\n",
    "- __\\_s2010-08-06T06:36:00__: This is the start time (`_st`) of the sample.\n",
    "- __\\_e2010-08-06T18:24:00__: This is the end time (`_et`) of the sample.\n",
    "\n",
    "The files that don't contain the @ symbol begin with FQ and do not have any flare occuring within 24 hours of the sample in the file.  __Note__ that both the __FL__ and __NF__ have files that have flares within 24 hours, but the __NF__ ones are smaller flares that we are considering as unimportant and therefore fall in the non-flaring class.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the flare and non-flare data \n",
    "---\n",
    "\n",
    "Now that you have an understanding about the data, you will develop a function to read the flaring and non-flaring data and return an object that contains the data from the csv file and some of the information contained in the file name.\n",
    "\n",
    "Below is the object you will return.\n",
    "\n",
    "Notice that it takes in several objects: a [string](https://docs.python.org/3/library/string.html), two [datetime](https://docs.python.org/3/library/datetime.html) objects, and a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTSSample:\n",
    "    \n",
    "    def __init__(self, flare_type:str, start_time:datetime, end_time:datetime, data:DataFrame):\n",
    "        self._flare_type = flare_type\n",
    "        self._start_time = start_time\n",
    "        self._end_time = end_time\n",
    "        self._data = data\n",
    "    \n",
    "    def get_flare_type(self):\n",
    "        return self._flare_type\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self._start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self._end_time\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the MVTSSample class\n",
    "---\n",
    "\n",
    "The above class represents the data contained in one file. You are to return one of these objects for each call to your method(s). \n",
    "\n",
    "- The __flare_type__ is to be one of the following selections (__X__, __M__, __C__, __B__, __FQ__), and these lables will be derived from the information in the file name. The label __FQ__ should be manually created for file names starting with the character `F`.\n",
    "- __start_time__ is the start time in the file name\n",
    "- __end_time__ is the end time in the file name\n",
    "- __data__ is a `Pandas DataFrame` which you will load from the csv using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About your method\n",
    "---\n",
    "\n",
    "Your method is to take in the path and name of the file to open, and it is to return one `MVTSSample` for that file.\n",
    "\n",
    "Below is a definition for that method, use it and write the code to complete the tasks necessary to return the specified information.  You can use a method call in another code block to test that your method works as required.\n",
    "\n",
    "Some useful methods/functions to use for this question are:\n",
    "\n",
    "* [String.find](https://www.w3schools.com/python/ref_string_find.asp)\n",
    "\n",
    "* [datetime.strptime](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)\n",
    "\n",
    "* [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) (__Note:__ the csv files are tab delimited so you will need to use `sep=\"\\t\"` to read them properly.)\n",
    "\n",
    "* [os.path.join](https://docs.python.org/3/library/os.path.html#os.path.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mvts_instance(data_dir:str, file_name:str) -> MVTSSample: #Finished!\n",
    "    # Get flare type from file name\n",
    "    flare_type = file_name[0:2]\n",
    "\n",
    "    try:\n",
    "        # Get start time from file name\n",
    "        start = file_name.find('s2')\n",
    "        start_time = file_name[start+1: start+20]\n",
    "        start_time = start_time.replace(\"T\", \" \")\n",
    "        start_time = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Get end time from file name\n",
    "        end = file_name.find('e2')\n",
    "        end_time = file_name[end+1: end+20]\n",
    "        end_time = end_time.replace(\"T\", \" \")\n",
    "        end_time = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "        pass\n",
    "\n",
    "    # Get data from csv file\n",
    "    try:\n",
    "        data = pd.read_csv(data_dir + \"/\" + file_name, sep=\"\\t\")\n",
    "    except ValueError:\n",
    "        print(ValueError)\n",
    "        pass\n",
    "    \n",
    "    # Make mvts object \n",
    "    mvts = MVTSSample(flare_type, start_time, end_time, data)\n",
    "    return mvts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/partition1/FL\"  # change the path to where your data is stored. (done)\n",
    "file_name = \"M1.0@265:Primary_ar115_s2010-08-06T06:36:00_e2010-08-06T18:24:00.csv\"\n",
    "results = read_mvts_instance(data_dir, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the DataFrame \n",
    "---\n",
    "\n",
    "Now that you can read individual files to get the multivariate time sries for a sample period, it is time to start building the analytics base table (ABT).\n",
    "\n",
    "The machine learning methods that we will cover are generally applied to tabular data with a set of descriptive features that are used to learn to classify or predict a target feature. To accomplish this with our raw input multivariate time series, we must produce a set of descriptive features from each of the variates of the the time series.  \n",
    "\n",
    "In this section you will process the DataFrame that was returned from your `read_mvts_instance` method to construct a set of descriptive features for each MVTS sample. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DataFrame Attributes:\n",
    "\n",
    "Above, you saw the 54 variates of the multivariate timeseries sample in each file. These 54 columns should be present in your dataframe that was returned from your previous `read_mvts_instance` method. For the next question, however, we will only be utilizing a fraction of those. The method description below gives you more information about which ones we will use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About your method\n",
    "---\n",
    "The following will be the variates we will be processing to return features of.\n",
    "\n",
    "|              |                  |             |\n",
    "|--------------|------------------|-------------|\n",
    "| 1. R_VALUE   | 2. TOTUSJH       | 3. TOTBSQ   |\t\n",
    "| 4. TOTPOT\t   | 5. TOTUSJZ       | 6. ABSNJZH  |\t\n",
    "| 7. SAVNCPP   | 8. USFLUX        | 9. TOTFZ\t|\n",
    "| 10. MEANPOT  | 11. EPSZ\t      | 12. MEANSHR |\n",
    "| 13. SHRGT45  | 14. MEANGAM      | 15. MEANGBT |\n",
    "| 16. MEANGBZ  | 17. MEANGBH      | 18. MEANJZH |\n",
    "| 19. TOTFY    | 20. MEANJZD      | 21. MEANALP |\t\n",
    "| 22. TOTFX    |        \t      |         \t|\n",
    "\n",
    "For each of these variates you will calculate two descriptive features: \n",
    "\n",
    "- Median \n",
    "- Standard Deviation\n",
    "\n",
    "Note:\n",
    "* Computing these 2 descriptive features on the 22 variates listed above should yield a dataframe of 44 columns. Make sure your implementation of `calculate_descriptive_features` has all those columns. We will add more later, but for now, this will be sufficient to demonstrate the analytics base table construction process.\n",
    "* The column names of your new dataframe should have both the variate name and the descriptive feature name (e.g., `TOTPOT_MEDIAN`).\n",
    "\n",
    "Below is a function defintion, complete it to return the above specified information. You can use a method call in another code block to test that your method works as required.\n",
    "\n",
    "Some useful methods/functions for this question are:\n",
    "\n",
    "* [numpy.median](https://numpy.org/doc/stable/reference/generated/numpy.median.html)\n",
    "\n",
    "* [numpy.std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)\n",
    "\n",
    "* [pandas.DataFrame.to_numpy](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy) (__Note:__ this should be used to get your selected column into a format that the numpy functions above require.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptive_features(data:DataFrame)-> DataFrame: #Finished!\n",
    "    variates_to_calc_on = [ 'R_VALUE','TOTUSJH','TOTBSQ','TOTPOT','TOTUSJZ','ABSNJZH','SAVNCPP',\n",
    "                           'USFLUX','TOTFZ','MEANPOT','EPSZ','MEANSHR','SHRGT45','MEANGAM','MEANGBT',\n",
    "                           'MEANGBZ','MEANGBH','MEANJZH','TOTFY','MEANJZD','MEANALP','TOTFX']\n",
    "    features_to_return = [ 'R_VALUE_MEDIAN','R_VALUE_STDDEV',\n",
    "                          'TOTUSJH_MEDIAN','TOTUSJH_STDDEV',\n",
    "                          'TOTBSQ_MEDIAN','TOTBSQ_STDDEV',\n",
    "                          'TOTPOT_MEDIAN','TOTPOT_STDDEV',\n",
    "                          'TOTUSJZ_MEDIAN','TOTUSJZ_STDDEV',\n",
    "                          'ABSNJZH_MEDIAN','ABSNJZH_STDDEV',\n",
    "                          'SAVNCPP_MEDIAN','SAVNCPP_STDDEV',\n",
    "                          'USFLUX_MEDIAN','USFLUX_STDDEV',\n",
    "                          'TOTFZ_MEDIAN','TOTFZ_STDDEV',\n",
    "                          'MEANPOT_MEDIAN','MEANPOT_STDDEV',\n",
    "                          'EPSZ_MEDIAN','EPSZ_STDDEV',\n",
    "                          'MEANSHR_MEDIAN','MEANSHR_STDDEV',\n",
    "                          'SHRGT45_MEDIAN','SHRGT45_STDDEV',\n",
    "                          'MEANGAM_MEDIAN','MEANGAM_STDDEV',\n",
    "                          'MEANGBT_MEDIAN','MEANGBT_STDDEV',\n",
    "                          'MEANGBZ_MEDIAN','MEANGBZ_STDDEV',\n",
    "                          'MEANGBH_MEDIAN','MEANGBH_STDDEV',\n",
    "                          'MEANJZH_MEDIAN','MEANJZH_STDDEV',\n",
    "                          'TOTFY_MEDIAN','TOTFY_STDDEV',\n",
    "                          'MEANJZD_MEDIAN','MEANJZD_STDDEV',\n",
    "                          'MEANALP_MEDIAN','MEANALP_STDDEV',\n",
    "                          'TOTFX_MEDIAN','TOTFX_STDDEV']\n",
    "    # Create empty data frame for return with named columns \n",
    "    df = pd.DataFrame(columns=features_to_return)\n",
    "\n",
    "    \n",
    "    # For each element append to temp list\n",
    "    list2add = []\n",
    "    for d in variates_to_calc_on:\n",
    "        l = data[d].to_numpy()\n",
    "        median = np.median(l)\n",
    "        std = np.std(l)\n",
    "        list2add.append(median)\n",
    "        list2add.append(std)\n",
    "        continue\n",
    "    \n",
    "    df.loc[len(df)] = list2add\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_descriptive_features(results.get_data()) # Use to test calcualted_descriptivefeatures function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Putting the pieces together \n",
    "\n",
    "---\n",
    "\n",
    "Now that you have the tools to read the data and process descriptive features, it is time to put this all together to produce an analytics base table for all of the data in Partiton 1.\n",
    "\n",
    "In this question, you shall construct a method that will process a partition by extracting features for each sample in both the __FL__ and __NF__ subdirectories of that partition. The extracted descriptive features (e.g., `TOTPOT_MEDIAN`) are to be placed into your analytics base table DataFrame as colums, with the addition of the `FLARE_TYPE` target feature.\n",
    "\n",
    "Note:\n",
    "* Your method should take in the partition location and assume that there will be __FL__ and __NF__ subdirectories to process.\n",
    "\n",
    "* Remember that your analytics base table should contain 5 flare types (`X`, `M`, `C`, `B`, and `FQ`).\n",
    "\n",
    "* Your method shall also take in the name of the analytics base table to store. This should be the full name with either an absolute or relative path to store the table also part of the passed in name. \n",
    "\n",
    "__Suggestion__: It would be a good idea to debug your function on a much smaller version of one partition (often claled a \"pet dataset\") and run it on the entire Partition 1 only when you are confident that it is error-free.\n",
    "\n",
    "Below you will find a method defintion, complete it to perform the above specified information. You can use a method call in another code block to test that your method works as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(partition_location:str, abt_name:str): #NEEDS WORK!\n",
    "    abt_header = [ 'FLARE_TYPE', 'R_VALUE_MEDIAN','R_VALUE_STDDEV',\n",
    "                          'TOTUSJH_MEDIAN','TOTUSJH_STDDEV',\n",
    "                          'TOTBSQ_MEDIAN','TOTBSQ_STDDEV',\n",
    "                          'TOTPOT_MEDIAN','TOTPOT_STDDEV',\n",
    "                          'TOTUSJZ_MEDIAN','TOTUSJZ_STDDEV',\n",
    "                          'ABSNJZH_MEDIAN','ABSNJZH_STDDEV',\n",
    "                          'SAVNCPP_MEDIAN','SAVNCPP_STDDEV',\n",
    "                          'USFLUX_MEDIAN','USFLUX_STDDEV',\n",
    "                          'TOTFZ_MEDIAN','TOTFZ_STDDEV',\n",
    "                          'MEANPOT_MEDIAN','MEANPOT_STDDEV',\n",
    "                          'EPSZ_MEDIAN','EPSZ_STDDEV',\n",
    "                          'MEANSHR_MEDIAN','MEANSHR_STDDEV',\n",
    "                          'SHRGT45_MEDIAN','SHRGT45_STDDEV',\n",
    "                          'MEANGAM_MEDIAN','MEANGAM_STDDEV',\n",
    "                          'MEANGBT_MEDIAN','MEANGBT_STDDEV',\n",
    "                          'MEANGBZ_MEDIAN','MEANGBZ_STDDEV',\n",
    "                          'MEANGBH_MEDIAN','MEANGBH_STDDEV',\n",
    "                          'MEANJZH_MEDIAN','MEANJZH_STDDEV',\n",
    "                          'TOTFY_MEDIAN','TOTFY_STDDEV',\n",
    "                          'MEANJZD_MEDIAN','MEANJZD_STDDEV',\n",
    "                          'MEANALP_MEDIAN','MEANALP_STDDEV',\n",
    "                          'TOTFX_MEDIAN','TOTFX_STDDEV']\n",
    "    \n",
    "    abt = pd.DataFrame(columns=abt_header)\n",
    "\n",
    "    # Get lists of data from partition\n",
    "    FL = os.listdir(partition_location + \"/FL\")\n",
    "    NF = os.listdir(partition_location + \"/NF\")\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    # Add row to abt from mvssample object and its median and std data\n",
    "    for d in FL + NF:\n",
    "\n",
    "        # Use temp list for each row and temp df\n",
    "        list2add = []\n",
    "        tempdf = pd.DataFrame(columns=abt_header)\n",
    "\n",
    "        # Get mvs object and add flare type \n",
    "        if d in FL:\n",
    "            mvs = read_mvts_instance(partition_location + '/FL', d)\n",
    "        else:\n",
    "            mvs = read_mvts_instance(partition_location + '/NF', d)\n",
    "        list2add.append(mvs.get_flare_type())\n",
    "\n",
    "        # Set up temp df for future concat with master data frame object\n",
    "        tempdf2 = calculate_descriptive_features(mvs.get_data())\n",
    "        templist = tempdf2.to_numpy()\n",
    "\n",
    "        # From data frame concat current with temp for each feature\n",
    "        for i in templist[0]:\n",
    "            list2add.append(i)\n",
    "            continue\n",
    "        tempdf.loc[45] = list2add\n",
    "        abt = pd.concat([abt, tempdf], ignore_index= True, axis = 0)\n",
    "\n",
    "        ''' Limit to 10000 files for testing'''\n",
    "        # count +=1\n",
    "        # if count >= 10000:\n",
    "        #     break\n",
    "        # continue\n",
    "    \n",
    "    \n",
    "    # return the completed analitics base table\n",
    "    return abt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/partition1\"  # change the path to where your data is stored.\n",
    "abt_name = \"partition1_features.csv\" # Corrected to partition 1\n",
    "abt = process_partition(data_dir, abt_name)\n",
    "print(abt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pandas.DataFrame.describe` function you check a few things including the total number of samples. Use that to ensure you have processed all MVTS samples of partition 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing the distribution of flares \n",
    "---\n",
    "\n",
    "How does the distribution of our 5 flare classes look like? This is the question we want to answer using a simple visualization. You can use the [pyplot.bar](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html) function from the [matplotlib](https://matplotlib.org/stable/index.html) library. The x-axis of the plot should represent the flare types and the y-axis should represent the counts of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create dictionary for counts of each flare type\n",
    "flare_types = {\"M\": 0, \"X\": 0, \"C\": 0, \"B\": 0, \"FQ\": 0}\n",
    "\n",
    "# For each objects flare type sort into M, X, C, B, or FQ, otherwise skip\n",
    "for i in abt['FLARE_TYPE']:\n",
    "    if \"M\" in i:\n",
    "        flare_types[\"M\"] +=1\n",
    "    elif \"X\" in i:\n",
    "        flare_types[\"X\"] +=1\n",
    "    elif \"C\" in i:\n",
    "        flare_types[\"C\"] +=1\n",
    "    elif \"B\" in i:\n",
    "        flare_types[\"B\"] +=1\n",
    "    elif \"FQ\" in i:\n",
    "        flare_types[\"FQ\"] +=1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "plt.bar(flare_types.keys(), flare_types.values())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

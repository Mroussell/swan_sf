{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "In this notebook you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "\n",
    "This notebook will have you explore the cardinalities, number of missing values, detect outliers, handle missing values and outliers, and create data quality report for original and cleaned dataset. Additionally, you will be asked to provide documentation for your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This notebook will only be using [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB). Recall that in NB1 we started to construct the analytics base table for our [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM). We will need to read the data from the two subdirectories, __FL__ and __NF__, of the __partition1__ direcotry. These two subdirectories represented the two classes of our target feature in the solar flare prediction problem we will be attempting to solve this semester. We then processed these samples of multivariate time series to construct descriptive features for each sample, and then placed them into our analytics base table.\n",
    "\n",
    "You will be utilizing a set of extracted descriptive features much like what you were asked to construct in NB1. However, this dataset contains many more extracted features than you were asked to compute for NB1 (>800), so we need to explore the data to find data quality issues and identify ways to address these issues. Below are links to the full extracted feature dataset for partition 1 and a toy dataset to use for testing you functions.\n",
    "\n",
    "__Note:__ Since the full dataset, and multiple copies of partially processed intermediary results, tend to take up a bit of space, you can use the toy dataset to implement and test your code. You may need to edit the data to fully test each of the requirements, but that is left as an exercise for the student. The full partition dataset is only included for those who wish to work with it once they have their code implemented. \n",
    "\n",
    "- [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv)\n",
    "- [Toy Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_partition1ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the extracted features csv files, you will load that data into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/MVTS'\n",
    "data_file = 'toy_partition1ExtractedFeatures.csv'\n",
    "full_data_file = 'partition1ExtractedFeatures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file), index_col=0)\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, full_data_file), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: Quality Report\n",
    "\n",
    "Write a function to extract the various pieces of a data quality report, for a specific attribute, and return a dataframe with this information.\n",
    "\n",
    " * 'Feature Name': Contains the time series statistical feature name\n",
    " \n",
    " * 'Cardinality': Contains the count of unique values for the feature\n",
    "            \n",
    " * 'Non-null Count': Contains the number of non-null entries for the feature\n",
    "            \n",
    " * 'Null Count': Contains the number of null or missing entries for the feature\n",
    "            \n",
    " * 'Min': Contains the minimum value of the feature (Without considering the null or nan value)\n",
    " \n",
    " * '25th': Contains the first quartile (25%) of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * 'Mean': Contains the mean of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * '50th': Contains the median of the feature values (Without considering the null/nan value)\n",
    "            \n",
    " * '75th': Contains the third quartile (75%) of the feature values (Without considering the null/nan value)\n",
    " \n",
    " * 'Max': Contains the maximum value of the feature (Without considering the null/nan value),\n",
    "            \n",
    " * 'Std. Dev': Contains the standard deviation of the feature (Without considering the null/nan value)\n",
    " \n",
    "In addition to the values above, you should identify the number of upper and lower outliers using the $val < Q1-1.5IQR$ and $val > Q3+1.5IQR$ outlier identification method. These added features should be called `Outlier Count Low` and `Outliers Count High` respectively.\n",
    "\n",
    "\n",
    " \n",
    " Some useful functions for this can be found at:\n",
    " \n",
    " * [Numpy.percentile](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html)\n",
    " \n",
    " * [pandas.isna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html)\n",
    " \n",
    " * [Numpy.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)\n",
    " \n",
    " * [Numpy.std](https://numpy.org/doc/stable/reference/generated/numpy.std.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summary_for(feature_name:str, data:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates Summary Features in list of 'summary_feature_names'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_name : str\n",
    "        Name of feature for summary to be calculated for. required\n",
    "    data : pandas.DataFrame object\n",
    "        A DataFrame object containg column named feature_name. required\n",
    "\n",
    "    Returns : DataFrame object of summary calculated features. \n",
    "    \"\"\"\n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max', 'Std. Dev','Outlier Count Low', 'Outlier Count High']\n",
    "    \n",
    "    # Create DataFrame to return and list at loc to DataFram\n",
    "    frame2return = pd.DataFrame(columns=summary_feature_names)\n",
    "    list2add = []\n",
    "    \n",
    "    # Find claculated features that have bult in functions\n",
    "    list2add.append(feature_name)\n",
    "    list2add.append(data.shape[0])\n",
    "    list2add.append(data[feature_name].notnull().sum())\n",
    "    list2add.append(data[feature_name].isnull().sum())\n",
    "    list2add.append(data[feature_name].min())\n",
    "    list2add.append(data[feature_name].quantile(q=0.25))\n",
    "    list2add.append(data[feature_name].mean())\n",
    "\n",
    "    # Save for Calcuating IQR\n",
    "    list2add.append(data[feature_name].quantile(q=0.5))\n",
    "    Q1 = data[feature_name].quantile(q=0.5)\n",
    "    list2add.append(data[feature_name].quantile(q=0.75))\n",
    "    Q3 = data[feature_name].quantile(q=0.75)\n",
    "\n",
    "    list2add.append(data[feature_name].max())\n",
    "    list2add.append(data[feature_name].std())\n",
    "    # Find IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Find Range for outliers\n",
    "    outerBoundLow = Q1 - (1.5 * IQR)\n",
    "    outerBoundHigh = Q3 + (1.5 * IQR)\n",
    "    \n",
    "\n",
    "    # Find calculated features using IQR counting outliers\n",
    "    countLow = 0\n",
    "    countHigh = 0\n",
    "    countInBounds = 0\n",
    "    for i in data[feature_name]:\n",
    "        if i < outerBoundLow:\n",
    "            countLow +=1\n",
    "        elif i > outerBoundHigh:\n",
    "            countHigh +=1\n",
    "        else :\n",
    "            countInBounds +=1\n",
    "    \n",
    "    list2add.append(countLow)\n",
    "    list2add.append(countHigh)\n",
    "\n",
    "    # Add list to Dataframe and return it\n",
    "    frame2return.loc[len(frame2return)] = list2add\n",
    "    return frame2return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_TOTUSJH_min = calc_summary_for('TOTUSJH_min', abt)\n",
    "print(summary_TOTUSJH_min) # Used for testing return from calc_summary_for()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: Report Construction\n",
    "Using what you produced to answere P1, you should now write a function to construct the data quality report for all of the numerical features of our dataset.  You should loop over all of the features in the analytics base table represented by the input feature dataset files from partition 1, with the exception of the first column (this is the index column if you read the file correctly), and the `id`, `lab`, `st`, and `et` columns.  \n",
    "\n",
    "Your output from this function will be a DataFrame that has 1 row for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_quality_report(data:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Uses calc_summary_for() to constuct full DataFrame of summary features for all features except 'excluded_columns'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame object\n",
    "        A DataFrame object with feature names as columns. required\n",
    "\n",
    "    Returns : DataFrame object of full quality summary calculated features. \n",
    "    \"\"\"\n",
    "\n",
    "    excluded_columns = ['id', 'lab', 'st', 'et']\n",
    "    \n",
    "    summary_feature_names = ['Feature Name', 'Cardinality', 'Non-null Count', 'Null Count', 'Min', '25th', 'Mean', \n",
    "                             '50th', '75th', 'Max', 'Std. Dev','Outlier Count Low', 'Outlier Count High']\n",
    "    \n",
    "    # Create DataFrame to return\n",
    "    frame2return = pd.DataFrame(columns=summary_feature_names)\n",
    "\n",
    "    # For each feature calc_summary_feat()\n",
    "    for col in data.head():\n",
    "        if col in excluded_columns:\n",
    "            continue\n",
    "        df2add = calc_summary_for(col, data)\n",
    "        frame2return = pd.concat([frame2return,df2add])\n",
    "    \n",
    "\n",
    "    return frame2return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = construct_quality_report(abt)\n",
    "print(summary_table.shape)  # checking the dimensionality is often a useful test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3: Drop features with improper cardinality:\n",
    "Using the quality report summary table that is returned from the function you wrote for P2, we are now going to investigate our data. For this, you should use the table returned for the [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv) and not the toy dataset I provided for testing.\n",
    "\n",
    "Since we are using real valued features, a majority of them shall have a cardinality close to the sample count. So, for this question, you are to write a function that takes in the summary table and the input dataset DataFrame, and drops the feature that have a cardinality less than 10. This feature should be dropped from both the data quality report summary table and from the actual input dataset DataFrame.\n",
    "\n",
    "A useful method for this operation is:\n",
    "\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) (Make sure to use the inplace option otherwise it returns a copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_card_data(summary_table:DataFrame, data:DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Drops features with Cardinality less than 10.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_table : pandas.DataFrame object\n",
    "        A DataFrame object containing calculated summary features. required\n",
    "    data : pandas.DataFrame object\n",
    "        A DataFrame object containing feature names as columns. required\n",
    "\n",
    "    Returns : Inplace Dataframe Objects with dropped features. \n",
    "    \"\"\"\n",
    "\n",
    "    # For each Feature check Cardinality \n",
    "    for feature in summary_table['Feature Name']:\n",
    "        if feature in str(summary_table['Feature Name']):      \n",
    "            tempdf = summary_table[summary_table['Feature Name'] == feature]\n",
    "            card = tempdf['Cardinality']\n",
    "\n",
    "            # If Cardinality less than 10 remove from summary \n",
    "            if int(card) < 10:\n",
    "                summary_table.drop(feature, inplace=True)\n",
    "                data.drop(columns=feature, inplace=True)\n",
    "        continue\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_low_card_data(summary_table, abt)\n",
    "print(summary_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4: Drop features with excessive NaN\n",
    "\n",
    "Again, using the quality report summary table that is returned from the function you wrote for P2, we are going to continue investigating our data. For this, you should still be using the table returned for the [Full Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/partition1ExtractedFeatures.csv) and not the toy dataset I provided for testing.\n",
    "\n",
    "Like the features that were dropped for P3, some of the extracted features don't work on all of the variates of the input multi-variate time series samples very well.  So, some of these features return an excessive number of `NaN` values.  These are not verry useful features, so we want to get rid of them before we continue. To do this, you are to write a function that takes in the summary table and the input dataset DataFrame, and drops the features that have **more than 1%** of the entries as null/nan values. Again, these features should be dropped from both the data quality report summary table and from the actual input dataset DataFrame.\n",
    "\n",
    "As in P3, a useful method for this operation is:\n",
    "\n",
    "* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) (Make sure to use the inplace option otherwise it returns a copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excessive_nan_data(summary_table:DataFrame, data:DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Drops features with Null Count less than 1% of Cardinality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_table : pandas.DataFrame object\n",
    "        A DataFrame object containing calculated summary features. required\n",
    "    data : pandas.DataFrame object\n",
    "        A DataFrame object containing feature names as columns. required\n",
    "\n",
    "    Returns : Inplace Dataframe Objects with dropped features. \n",
    "    \"\"\"\n",
    "    \n",
    "    # For each Feature check Null Count against Cardinlaity\n",
    "    for feature in summary_table['Feature Name']:\n",
    "        if feature in str(summary_table['Feature Name']):      \n",
    "            tempdf = summary_table[summary_table['Feature Name'] == feature]\n",
    "            card = int(tempdf['Cardinality'])\n",
    "            null_count = int(tempdf['Null Count'])\n",
    "\n",
    "            # If Null Count > 1% Cardinality than  remove from summary \n",
    "            if null_count > (.01 * card):\n",
    "                summary_table.drop(feature, inplace=True)\n",
    "                data.drop(columns=feature, inplace=True)\n",
    "        continue\n",
    "\n",
    "    return summary_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_excessive_nan_data(summary_table, abt)\n",
    "print(summary_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the data cleaned up a little, save the results of both your summary table and your analytics base table using the [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) method. We will want to use these results for the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data/MVTS/' # Edited For Unix Machine \n",
    "out_summary_table_name = 'data_quality_table.csv'\n",
    "out_abt_name = 'cleaned_partition1ExtractedFeatures.csv'\n",
    "\n",
    "# Testing Full Data\n",
    "summary_table2 = construct_quality_report(abt2)\n",
    "drop_low_card_data(summary_table2, abt2)\n",
    "drop_excessive_nan_data(summary_table2, abt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Summary and Cleaned Data CSVs\n",
    "summary_table.to_csv(out_dir + out_summary_table_name)\n",
    "abt2.to_csv(out_dir + out_abt_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

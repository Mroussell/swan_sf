{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verified-pulse",
   "metadata": {},
   "source": [
    "# Notebook 5: Model Evaluation\n",
    "As in the previous notebooks, in this  notebook you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "\n",
    "This notebook will utilize a copy of the extracted feature dataset we have been working with. The dataset has been processed by performing outlier clipping, z-score and range scaling, and forward feature selection to select 20 features. We are now going to utilize more than one partition worth of data, so for the z-score and range scaling, the mean, standard deviation, minimum, and maximum were calculated using data from both partitions so that a global scaling can be performed on each partition. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-photography",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This notebook will continue to only use [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) and will add the use of [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) as a testing set. \n",
    "\n",
    "---\n",
    "\n",
    "For this notebook, cleaning, transforming, and normalization of the data has been completed using both partitions to find the various minimum, maximum, standard deviation, and mean values needed to perform these operations. Recall from lecture that we should not perform these operations on each partition individually, but as a whole as there may(will) be different values for these in different partitions. \n",
    "\n",
    "For example, if we perform simple range scaling on each partition individually and we see a range of 0 to 100 in one partition and 0 to 10 in another. After individual scaling the values with 100 in the first would be mapped to 1 just like the values that had 10 in the second. This can cause serious performance problems in your model, so I have made sure that the normalization was treated properly for you. \n",
    "\n",
    "Below you will find the full partitions and `toy` sampled data from each partition, where only 20 samples from each of our 5 classes have been included in the data.  \n",
    "\n",
    "#### Full\n",
    "- [Full Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition1ExtractedFeatures.csv)\n",
    "- [Full Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "#### Toy\n",
    "- [Toy Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition1ExtractedFeatures.csv)\n",
    "- [Toy Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the two files, you should load each into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-gather",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "For each of the models we evaluate in this assignmnet, you will calculate the True Skill Statistic score using the test data from Partition 2 to determine which model performs the best for classifying the positive flaring class.\n",
    "\n",
    "    True skill statistic (TSS) = TPR + TNR - 1 = TPR - (1-TNR) = TPR - FPR\n",
    "\n",
    "Where:\n",
    "\n",
    "    True positive rate (TPR) = TP/(TP+FN) Also known as recall or sensitivity\n",
    "    True negative rate (TNR) = TN/(TN+FP) Also known as specificity or selectivity\n",
    "    False positive rate (FPR) = FP/(FP+TN) = (1-TNR) Also known as fall-out or false alarm ratio\n",
    "\n",
    "\n",
    "**Recall**\n",
    "\n",
    "    True positive (TP)\n",
    "    True negative (TN)\n",
    "    False positive (FP)\n",
    "    False negative (FN)\n",
    "    \n",
    "See [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for more information.\n",
    "\n",
    "Below is a function implemented to provide your score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "educated-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eastern-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tss(y_true=None, y_predict=None):\n",
    "    \"\"\"\n",
    "    Calculates the true skill score for binary classification based on the output of the confusion\n",
    "    table function\n",
    "    \"\"\"\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    \n",
    "    return tp_rate - fp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d9080",
   "metadata": {},
   "source": [
    "As in the previous notebook, we will be utilizing a binary classification of our 5 class dataset. So, below is the helper function to change our class labels from the 5 class target feature to the binary target feature. The function is implemented to take a dataframe (e.g. our `abt`) and prepares it for a binary classification by merging the `X`- and `M`-class samples into one group, and the rest (`NF`, `B`, and `C`) into another group, labeled with `1`s and `0`s, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d974d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomize_X_y(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    dichotomizes the dataset and split it into the features (X) and the labels (y).\n",
    "    \n",
    "    :return: two np.ndarray objects X and y.\n",
    "    \"\"\"\n",
    "    data_dich = data.copy()\n",
    "    data_dich['lab'] = data_dich['lab'].map({'NF': 0, 'B': 0, 'C': 0, 'M': 1, 'X': 1})\n",
    "    y = data_dich['lab']\n",
    "    X = data_dich.drop(['lab'], axis=1)\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "stable-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/FDS/'\n",
    "data_file = \"toy_normalized_partition1ExtractedFeatures.csv\"\n",
    "data_file2 = \"toy_normalized_partition2ExtractedFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "qualified-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file))\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, data_file2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-category",
   "metadata": {},
   "source": [
    "---\n",
    "### P1 \n",
    "\n",
    "Just like you did with the previous notebook, you will be utilizing a few different types of feature selection to find subsets of descriptive features to use in the models we will be evaluating.  For this part you will again be utilizing the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). You will then be using 3 diferent feature evaluation functions.\n",
    "\n",
    "-  [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n",
    "\n",
    "- [scikit-learn mutual_info_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n",
    "\n",
    "- [chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)\n",
    "\n",
    "For each of these combinations of evaluation functions, you need to construct a 20 feature training and testing dataset. This will be done by:\n",
    "<ol>\n",
    "    <li>Use the `SelectKBest` class with each of the evaluation functions to perform feature selection using Partition 1 as your input data</li>\n",
    "    <li>Construct a new train `DataFrame` for each instance of `SelectKBest` from 1 with the `lab` class labels using Partition 1</li>\n",
    "    <li>Construct a new test `DataFrame` for each instance of `SelectKBest` from 1 with the `lab` class labels using Partition 2</li>\n",
    "</ol>\n",
    "\n",
    "After this part, you should have a total of 6 `DataFrame`s to use in later parts, a train and test pair for each feature selection method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "relevant-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "abt_cpy = abt.copy()\n",
    "abt_cpy2 = abt2.copy()\n",
    "X, y = dichotomize_X_y(abt_cpy)\n",
    "X2, y2 = dichotomize_X_y(abt_cpy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8eda86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "##### f_classif features choices\n",
    "select = SelectKBest(score_func=f_classif, k=numFeat)\n",
    "select.fit(X, y)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats1 = pd.DataFrame(abt_cpy.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats1 = choosen_feats1.join(abt_cpy.iloc[:, i+1])\n",
    "\n",
    "# train1\n",
    "train1 = choosen_feats1\n",
    "del choosen_feats1\n",
    "\n",
    "select = SelectKBest(score_func=f_classif, k=numFeat)\n",
    "select.fit(X2, y2)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats1 = pd.DataFrame(abt_cpy2.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats1 = choosen_feats1.join(abt_cpy2.iloc[:, i+1])\n",
    "\n",
    "# test1\n",
    "test1 = choosen_feats1\n",
    "\n",
    "##### mutual_into_classif choices\n",
    "select = SelectKBest(score_func=mutual_info_classif, k=numFeat)\n",
    "select.fit(X, y)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats2 = pd.DataFrame(abt_cpy.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats2 = choosen_feats2.join(abt_cpy.iloc[:, i+1])\n",
    "\n",
    "# train2\n",
    "train2 = choosen_feats2\n",
    "del choosen_feats2\n",
    "\n",
    "select = SelectKBest(score_func=mutual_info_classif, k=numFeat)\n",
    "select.fit(X2, y2)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats2 = pd.DataFrame(abt_cpy2.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats2 = choosen_feats2.join(abt_cpy2.iloc[:, i+1])\n",
    "\n",
    "# test2\n",
    "test2 = choosen_feats2\n",
    "\n",
    "\n",
    "###### chi2 choices\n",
    "select = SelectKBest(score_func=chi2, k=numFeat)\n",
    "select.fit(X, y)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats3 = pd.DataFrame(abt_cpy.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats3 = choosen_feats3.join(abt_cpy.iloc[:, i+1])\n",
    "\n",
    "# train3\n",
    "train3 = choosen_feats3\n",
    "del choosen_feats3\n",
    "\n",
    "select = SelectKBest(score_func=chi2, k=numFeat)\n",
    "select.fit(X2, y2)\n",
    "feats_to_check = select.get_support()\n",
    "\n",
    "choosen_feats3 = pd.DataFrame(abt_cpy2.iloc[:, 0])\n",
    "for i, b in enumerate(feats_to_check):\n",
    "    if b:\n",
    "        choosen_feats3 = choosen_feats3.join(abt_cpy2.iloc[:, i+1])\n",
    "\n",
    "# test 3\n",
    "test3 = choosen_feats3\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-reynolds",
   "metadata": {},
   "source": [
    "---\n",
    "### P2 \n",
    "\n",
    "Now that we have our training and testing datasets for each of our feature subsets, we need to attempt to perform hyperparameter tuning on our model for each of the datasets. We want to see which combination of dataset and parameter settings seem to provide the best results. \n",
    "\n",
    "In order to do this, we must first dichotomize the training and testing data. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed in P1.  \n",
    "\n",
    "With your binary classification dataset constructed, now it's time to start training and testing some models. We will start with the simple [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and try several different settings to see how/if using different settings will improve our score. So, for each of your three copies of the Partition 1 training datasets that have had their `lab` columns converted to a binary label, train 4 different instances with the following settings. **(see documentation to know what these are)** In total you will train and evaluate 12 model setting and feature selected data pairings. \n",
    "\n",
    "|Model Number| n_neighbors | p |\n",
    "|------------|-------------|---|\n",
    "|1|3|1|\n",
    "|2|3|2|\n",
    "|3|5|1|\n",
    "|4|5|2|\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result. **NOTE: The model does take a little while to evaluate.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "direct-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "p = [1,2]\n",
    "temp = [n_neighbors, p]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6f5cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.85\n",
      "TSS:  0.8250000000000001\n",
      "TSS:  0.725\n",
      "TSS:  0.725\n",
      "TSS:  0.7583333333333333\n",
      "TSS:  0.725\n",
      "TSS:  0.775\n",
      "TSS:  0.7666666666666667\n",
      "TSS:  0.75\n",
      "TSS:  0.775\n",
      "TSS:  0.75\n",
      "TSS:  0.725\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "def knear(data, temp):\n",
    "   \n",
    "    X, y = dichotomize_X_y(data)\n",
    "\n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][0], p=temp[1][0])\n",
    "    n.fit(X, y)\n",
    "    y_pred1 = n.predict(X)\n",
    "    \n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][0], p=temp[1][1])\n",
    "    n.fit(X, y)\n",
    "    y_pred2 = n.predict(X)\n",
    "\n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][1], p=temp[0][0])\n",
    "    n.fit(X, y)\n",
    "    y_pred3 = n.predict(X)\n",
    "\n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][1], p=temp[1][1])\n",
    "    n.fit(X, y)\n",
    "    y_pred4 = n.predict(X)\n",
    "\n",
    "    return y_pred1, y_pred2, y_pred3, y_pred4 \n",
    "    \n",
    "\n",
    "# get y_preds\n",
    "y_pred1_3_1, y_pred1_3_2, y_pred1_5_1, y_pred1_5_2 = knear(train1, temp)\n",
    "y_pred2_3_1, y_pred2_3_2, y_pred2_5_1, y_pred2_5_2 = knear(train2, temp)\n",
    "y_pred3_3_1, y_pred3_3_2, y_pred3_5_1, y_pred3_5_2 = knear(train3, temp)\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "# Get confusion matrices\n",
    "cm_list = []\n",
    "cm131 = confusion_matrix(y1_test, y_pred1_3_1)\n",
    "cm_list.append(cm131)\n",
    "cm132 = confusion_matrix(y1_test, y_pred1_3_2)\n",
    "cm_list.append(cm132)\n",
    "cm151 = confusion_matrix(y1_test, y_pred1_5_1)\n",
    "cm_list.append(cm151)\n",
    "cm152 = confusion_matrix(y1_test, y_pred1_5_2)\n",
    "cm_list.append(cm152)\n",
    "cm231 = confusion_matrix(y2_test, y_pred2_3_1)\n",
    "cm_list.append(cm231)\n",
    "cm232 = confusion_matrix(y2_test, y_pred2_3_2)\n",
    "cm_list.append(cm232)\n",
    "cm251 = confusion_matrix(y2_test, y_pred2_5_1)\n",
    "cm_list.append(cm251)\n",
    "cm252 = confusion_matrix(y2_test, y_pred2_5_2)\n",
    "cm_list.append(cm252)\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_1)\n",
    "cm_list.append(cm331)\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_2)\n",
    "cm_list.append(cm331)\n",
    "cm351 = confusion_matrix(y3_test, y_pred3_5_1)\n",
    "cm_list.append(cm351)\n",
    "cm352 = confusion_matrix(y3_test, y_pred3_5_2)\n",
    "cm_list.append(cm352)\n",
    "\n",
    "# get TSSs\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-rating",
   "metadata": {},
   "source": [
    "---\n",
    "### P3 (10 points)\n",
    "\n",
    "After evaluating the various results from P2, you will notice that the results are not all that great with greater than 1000 false negatives for nearly all of our settings tried. But, what can be done to improve our results? If you read the documentation for the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), which you certainly should have, you will see that we were only using the `MinkowskiDistance` metric with different values of `p`. If you look into the [DistanceMetric](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric) documentation for the neighbors classifiers, you will see there are several others available to use.\n",
    "\n",
    "So, for this part, train and evaluate two more instances of [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for each of our different feature selection train test datsets, but this time using the `ChebyshevDistance` metric instead of the `MinkowskiDistance` metric.  For these models you will only be changing the number neighbors to 3 and 5, as the values of `p` are not used for the `ChebyshevDistance` metric. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dutch-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "temp = [n_neighbors]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "czech-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.8\n",
      "TSS:  0.7833333333333333\n",
      "TSS:  0.7833333333333333\n",
      "TSS:  0.7166666666666667\n",
      "TSS:  0.8583333333333334\n",
      "TSS:  0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    " #----------------------------------------------\n",
    "def knear_cd(data, temp):\n",
    "   \n",
    "    X, y = dichotomize_X_y(data)\n",
    "\n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][0], metric='chebyshev')\n",
    "    n.fit(X, y)\n",
    "    y_pred1 = n.predict(X)\n",
    "    \n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][1], metric='chebyshev')\n",
    "    n.fit(X, y)\n",
    "    y_pred2 = n.predict(X)\n",
    "\n",
    "    return y_pred1, y_pred2\n",
    "\n",
    "# get y_preds\n",
    "y_pred1_3_1, y_pred1_5_1,= knear_cd(train1, temp)\n",
    "y_pred2_3_1, y_pred2_5_1 = knear_cd(train2, temp)\n",
    "y_pred3_3_1, y_pred3_5_1 = knear_cd(train3, temp)\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "# Get confusion matrices\n",
    "cm_list = []\n",
    "cm131 = confusion_matrix(y1_test, y_pred1_3_1)\n",
    "cm_list.append(cm131)\n",
    "\n",
    "cm151 = confusion_matrix(y1_test, y_pred1_5_1)\n",
    "cm_list.append(cm151)\n",
    "\n",
    "cm231 = confusion_matrix(y2_test, y_pred2_3_1)\n",
    "cm_list.append(cm231)\n",
    "\n",
    "cm251 = confusion_matrix(y2_test, y_pred2_5_1)\n",
    "cm_list.append(cm251)\n",
    "\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_1)\n",
    "cm_list.append(cm331)\n",
    "\n",
    "cm351 = confusion_matrix(y3_test, y_pred3_5_1)\n",
    "cm_list.append(cm351)\n",
    "\n",
    "\n",
    "# get TSSs\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-large",
   "metadata": {},
   "source": [
    "---\n",
    "### P4 (10 points)\n",
    "\n",
    "After evaluating the results from P3, you will see that the results are no better than those we found for P2. This leads to the thought that maybe the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) is just not a good fit for the problem we are applying it to. So, let's move on to another classifier for this problem. \n",
    "\n",
    "In this part, you will utilize the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and try several different settings to see how/if using different settings will improve our score. So, continuing to use our training/testing pairs constructed with different feature selection methods that have had their `lab` column converted to a binary label, train 8 different instances with the following settings. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| criterion | max_depth | splitter |\n",
    "|------------|---------|-------------|---|\n",
    "|1|gini|5|best|\n",
    "|2|gini|5|random|\n",
    "|3|gini|None|best|\n",
    "|4|gini|None|random|\n",
    "|5|entropy|5|best|\n",
    "|6|entropy|5|random|\n",
    "|7|entropy|None|best|\n",
    "|8|entropy|None|random|\n",
    "\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "accredited-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "depth = [5, None]\n",
    "splitter = ['best', 'random']\n",
    "temp = [criterion, depth, splitter]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "documented-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.9666666666666667\n",
      "TSS:  0.875\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n",
      "TSS:  0.95\n",
      "TSS:  0.9\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n",
      "TSS:  0.95\n",
      "TSS:  0.9583333333333333\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n",
      "TSS:  0.975\n",
      "TSS:  0.8916666666666666\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n",
      "TSS:  0.95\n",
      "TSS:  0.8583333333333334\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n",
      "TSS:  0.9583333333333333\n",
      "TSS:  0.8333333333333333\n",
      "TSS:  1.0\n",
      "TSS:  1.0\n"
     ]
    }
   ],
   "source": [
    "def dec_tree(data, temp):\n",
    "    \n",
    "    X, y = dichotomize_X_y(data)\n",
    "    m = []\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][0], max_depth=temp[1][0], splitter=temp[2][0])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][0], max_depth=temp[1][0], splitter=temp[2][1])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][0], max_depth=temp[1][1], splitter=temp[2][0])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][0], max_depth=temp[1][1], splitter=temp[2][1])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    ############\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][1], max_depth=temp[1][0], splitter=temp[2][0])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][1], max_depth=temp[1][0], splitter=temp[2][1])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][1], max_depth=temp[1][1], splitter=temp[2][0])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=temp[0][1], max_depth=temp[1][1], splitter=temp[2][1])\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    m.append(y_pred)\n",
    "\n",
    "    return m\n",
    "\n",
    "# get models \n",
    "\n",
    "dt_models1 = dec_tree(train1, temp)\n",
    "dt_models2 = dec_tree(train2, temp)\n",
    "dt_models3 = dec_tree(train3, temp)\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "\n",
    "del cm_list\n",
    "cm_list = []\n",
    "\n",
    "# get confusion_matrices\n",
    "for model in dt_models1:\n",
    "    cm = confusion_matrix(y1_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for model in dt_models2:\n",
    "    cm = confusion_matrix(y2_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for model in dt_models3:\n",
    "    cm = confusion_matrix(y3_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-finder",
   "metadata": {},
   "source": [
    "---\n",
    "### P5 \n",
    "\n",
    "After evaluating results from P4, you will see that the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) was able to accomplish a bit of an improvement over the best resutls we found for the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).  This is indeed great, but can we do better than this if we use yet another classifier? Let's move on to yet another and find out.\n",
    "\n",
    "For this part you will be utilizing the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier. We won't be changing any of the default settings, just train 1 model for each of our feature selected data subsets. You will again be using your training/testing pairs constructed with different feature selection methods that have had their `lab` column converted to a binary label. You will then test each of your models using your binary classification copy of the Partition 2 testing dataset that was cunstructed with the same features the model was trained on. You shall then calculate and print the TSS score for each result.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "legitimate-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.7666666666666667\n",
      "TSS:  0.5333333333333333\n",
      "TSS:  0.6\n"
     ]
    }
   ],
   "source": [
    "def gnb(data):\n",
    "\n",
    "    X, y = dichotomize_X_y(data)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "# get y_preds models\n",
    "gnb_models = []\n",
    "gnb_models.append(gnb(train1))\n",
    "gnb_models.append(gnb(train2))\n",
    "gnb_models.append(gnb(train3))\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "# get confusion matrices\n",
    "del cm_list \n",
    "cm_list = []\n",
    "cm1 = confusion_matrix(y1_test, gnb_models[0])\n",
    "cm_list.append(cm1)\n",
    "cm2 = confusion_matrix(y1_test, gnb_models[1])\n",
    "cm_list.append(cm2)\n",
    "cm3 = confusion_matrix(y1_test, gnb_models[2])\n",
    "cm_list.append(cm3)\n",
    "\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-religion",
   "metadata": {},
   "source": [
    "---\n",
    "### P6\n",
    "\n",
    "If you recall from a lecture some time back, it was shown that another way of improving the results of classification is to perform some form of sampling to balance the number of samples there are for the various classes. The reason why this works for specific classifiers, and methods for doing the sampling, are numerious and we don't have enough time to cover all of them in this course.  However, it is still beneficial to know this works and that it is something that you should be considering when you are training models.  \n",
    "\n",
    "So, for this part, we will implement a very naive method for sampling so we can use the results for training our models again.  Below you will find a function stub, complete the function and have it return a copy of the input dataframe where each class (except for the smallest one) have been undersampled to match the size of the smallest class in the dataset. In this function you should assume the `lab` column is the class label and not the dicotomized binary classification converted label.\n",
    "\n",
    "To do this you may want to use the [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function of the DataFrame to get groups of rows from your DataFrame.  You may also wish to use the [sample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) function to select a number of rows from a group. You can also use the [apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method to process each group from your grouped rows. These are just hints, you can solve the problem how you see fit.\n",
    "\n",
    "Once this function is complete, apply it to each of your training datasets that have been constructed with different feature selection methods from partition 1 (the ones with all the NF, C, .., X labels). You will not be applying this to your testing sets. After you have your sampled feature selected datasets, you will then apply your function that converts the multi-class problem to a binary problem to each of the resultant selected subsets so we can use these new undersampled data for the next several parts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "qualified-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_under_sample(data:DataFrame)->DataFrame:\n",
    "    #----------------------------------------------\n",
    "    ## Group and find indicies of each type of lab\n",
    "    df = data.copy()\n",
    "    # df.groupby(by=['lab'])\n",
    "    B = df[df.lab == 'B'].index.values\n",
    "    C = df[df.lab == 'C'].index.values\n",
    "    M = df[df.lab == 'M'].index.values\n",
    "    NF = df[df.lab == 'NF'].index.values\n",
    "    X = df[df.lab == 'X'].index.values\n",
    "\n",
    "    ## gather all indices of nonflaring and flaring indices\n",
    "    nonflaring = np.concatenate((NF, B, C))\n",
    "    flaring = np.concatenate((X, M))\n",
    "    random_indices = np.random.choice(nonflaring, len(flaring), replace=True)\n",
    "\n",
    "    # undersample \n",
    "    under_sample_indices = np.concatenate((nonflaring,random_indices))\n",
    "    under_sample = df.loc[under_sample_indices]\n",
    "\n",
    "    return under_sample\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d1a47c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lab  ABSNJZH_var  SAVNCPP_median  SAVNCPP_mean  SAVNCPP_var  \\\n",
      "60  NF     0.626705        0.882830      0.884792     0.911350   \n",
      "61  NF     0.512074        0.896668      0.895379     0.898215   \n",
      "62  NF     0.525705        0.871603      0.872441     0.897351   \n",
      "63  NF     0.708280        0.899064      0.899471     0.940299   \n",
      "64  NF     0.248296        0.805085      0.808213     0.836438   \n",
      "..  ..          ...             ...           ...          ...   \n",
      "25   C     0.565503        0.878911      0.884029     0.913592   \n",
      "20   C     0.593854        0.890877      0.890867     0.921325   \n",
      "6    B     0.515842        0.878527      0.877806     0.898744   \n",
      "22   C     0.567849        0.891361      0.892639     0.900660   \n",
      "8    B     0.672086        0.945807      0.945975     0.930426   \n",
      "\n",
      "    SAVNCPP_linear_weighted_average  SAVNCPP_quadratic_weighted_average  \\\n",
      "60                         0.882273                            0.880605   \n",
      "61                         0.894074                            0.893747   \n",
      "62                         0.871817                            0.872033   \n",
      "63                         0.886372                            0.876859   \n",
      "64                         0.809268                            0.810164   \n",
      "..                              ...                                 ...   \n",
      "25                         0.888059                            0.890934   \n",
      "20                         0.892632                            0.895462   \n",
      "6                          0.876915                            0.876991   \n",
      "22                         0.890115                            0.889322   \n",
      "8                          0.946010                            0.945970   \n",
      "\n",
      "    TOTUSJH_max  TOTUSJH_linear_weighted_average  \\\n",
      "60     0.305399                         0.278971   \n",
      "61     0.151905                         0.145631   \n",
      "62     0.188589                         0.181689   \n",
      "63     0.165525                         0.141257   \n",
      "64     0.038821                         0.031177   \n",
      "..          ...                              ...   \n",
      "25     0.153086                         0.143040   \n",
      "20     0.360905                         0.356464   \n",
      "6      0.225538                         0.217458   \n",
      "22     0.233170                         0.225367   \n",
      "8      0.479244                         0.464807   \n",
      "\n",
      "    TOTUSJH_quadratic_weighted_average  ...  ABSNJZH_max  ABSNJZH_median  \\\n",
      "60                            0.286761  ...     0.162709        0.121123   \n",
      "61                            0.146158  ...     0.138837        0.120718   \n",
      "62                            0.181519  ...     0.102370        0.059941   \n",
      "63                            0.136955  ...     0.193783        0.143607   \n",
      "64                            0.031668  ...     0.034622        0.020665   \n",
      "..                                 ...  ...          ...             ...   \n",
      "25                            0.144714  ...     0.153848        0.120842   \n",
      "20                            0.356406  ...     0.139285        0.109406   \n",
      "6                             0.217634  ...     0.128682        0.108993   \n",
      "22                            0.225562  ...     0.154263        0.108723   \n",
      "8                             0.462252  ...     0.244624        0.217597   \n",
      "\n",
      "    ABSNJZH_mean  ABSNJZH_stddev  ABSNJZH_difference_of_means  \\\n",
      "60      0.120951        0.220220                     0.164337   \n",
      "61      0.121542        0.138370                     0.096236   \n",
      "62      0.067898        0.146232                     0.093595   \n",
      "63      0.142556        0.306523                     0.296480   \n",
      "64      0.022514        0.047290                     0.019544   \n",
      "..           ...             ...                          ...   \n",
      "25      0.125654        0.171835                     0.133326   \n",
      "20      0.106662        0.192763                     0.105384   \n",
      "6       0.110223        0.140500                     0.077356   \n",
      "22      0.114097        0.173477                     0.157364   \n",
      "8       0.215636        0.264695                     0.150929   \n",
      "\n",
      "    ABSNJZH_difference_of_medians  ABSNJZH_linear_weighted_average  \\\n",
      "60                       0.175931                         0.129365   \n",
      "61                       0.095175                         0.116091   \n",
      "62                       0.109378                         0.061815   \n",
      "63                       0.306242                         0.113227   \n",
      "64                       0.018465                         0.022943   \n",
      "..                            ...                              ...   \n",
      "25                       0.141871                         0.128033   \n",
      "20                       0.114660                         0.102456   \n",
      "6                        0.068401                         0.105535   \n",
      "22                       0.136520                         0.104471   \n",
      "8                        0.190519                         0.218910   \n",
      "\n",
      "    ABSNJZH_quadratic_weighted_average  ABSNJZH_last_value  SAVNCPP_stddev  \n",
      "60                            0.132418            0.151937        0.056891  \n",
      "61                            0.113956            0.118584        0.037203  \n",
      "62                            0.057623            0.052147        0.036178  \n",
      "63                            0.094732            0.027144        0.145073  \n",
      "64                            0.023673            0.029239        0.005047  \n",
      "..                                 ...                 ...             ...  \n",
      "25                            0.129934            0.113744        0.061168  \n",
      "20                            0.101105            0.140004        0.078547  \n",
      "6                             0.103141            0.119076        0.037845  \n",
      "22                            0.100533            0.094510        0.040264  \n",
      "8                             0.217874            0.190841        0.105422  \n",
      "\n",
      "[100 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "under_train1 = perform_under_sample(train1)\n",
    "under_train2 = perform_under_sample(train1)\n",
    "under_train3 = perform_under_sample(train1)\n",
    "print(under_train1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-signal",
   "metadata": {},
   "source": [
    "---\n",
    "### P7\n",
    "\n",
    "For this part repeat what you did for P2, but with your balanced binary classification datasets constructed in P6, uese the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), and try several different settings to see how/if using different settings will improve our score. \n",
    "\n",
    "So, train 4 different instances with the following settings for each of your feature selected subsets, for a total of 12 different evaluations. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| n_neighbors | p |\n",
    "|------------|-------------|---|\n",
    "|1|3|1|\n",
    "|2|3|2|\n",
    "|3|5|1|\n",
    "|4|5|2|\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (these should not have been balanced). You shall then calculate and print the TSS score for each result. **NOTE: The model now takes less time to evaluate!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "reported-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "p = [1,2]\n",
    "temp = [n_neighbors, p]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "oriental-permit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# get y_preds\n",
    "y_pred1_3_1, y_pred1_3_2, y_pred1_5_1, y_pred1_5_2 = knear(under_train1, temp)\n",
    "y_pred2_3_1, y_pred2_3_2, y_pred2_5_1, y_pred2_5_2 = knear(under_train2, temp)\n",
    "y_pred3_3_1, y_pred3_3_2, y_pred3_5_1, y_pred3_5_2 = knear(under_train3, temp)\n",
    "\n",
    "# get y_test and make same size\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "\n",
    "# Get confusion matrices\n",
    "cm_list = []\n",
    "cm131 = confusion_matrix(y1_test, y_pred1_3_1)\n",
    "cm_list.append(cm131)\n",
    "cm132 = confusion_matrix(y1_test, y_pred1_3_2)\n",
    "cm_list.append(cm132)\n",
    "cm151 = confusion_matrix(y1_test, y_pred1_5_1)\n",
    "cm_list.append(cm151)\n",
    "cm152 = confusion_matrix(y1_test, y_pred1_5_2)\n",
    "cm_list.append(cm152)\n",
    "cm231 = confusion_matrix(y2_test, y_pred2_3_1)\n",
    "cm_list.append(cm231)\n",
    "cm232 = confusion_matrix(y2_test, y_pred2_3_2)\n",
    "cm_list.append(cm232)\n",
    "cm251 = confusion_matrix(y2_test, y_pred2_5_1)\n",
    "cm_list.append(cm251)\n",
    "cm252 = confusion_matrix(y2_test, y_pred2_5_2)\n",
    "cm_list.append(cm252)\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_1)\n",
    "cm_list.append(cm331)\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_2)\n",
    "cm_list.append(cm331)\n",
    "cm351 = confusion_matrix(y3_test, y_pred3_5_1)\n",
    "cm_list.append(cm351)\n",
    "cm352 = confusion_matrix(y3_test, y_pred3_5_2)\n",
    "cm_list.append(cm352)\n",
    "\n",
    "# get TSSs\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-finland",
   "metadata": {},
   "source": [
    "---\n",
    "### P8\n",
    "\n",
    "After evaluating the various results from P7, you will notice that some of the results are improved over the same experiments we conducted in P2. Additionally, you should also notice a improvement in the speed at which the results were obtained. The question now is will we continue to see these improvements for all of our experiments? So, let's move on and see.\n",
    "\n",
    "For this question, you will repeat the experiments from P3, but using the balanced binary classification datasets constructed in P6. You will still be using the [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) like you did in P7, but you will again be changing from using the `MinkowskiDistance` metric with different values of `p` to using the `ChebyshevDistance` metric. You will construct two models for each of your feature selected datasets by changing the number neighbors to 3 and 5.\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (these should not have been balanced), then calculate and print the TSS score for each result. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "flush-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [3, 5]\n",
    "temp = [n_neighbors]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "better-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "def knear_cd(data, temp):\n",
    "   \n",
    "    X, y = dichotomize_X_y(data)\n",
    "\n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][0], metric='chebyshev')\n",
    "    n.fit(X, y)\n",
    "    y_pred1 = n.predict(X)\n",
    "    \n",
    "    n = KNeighborsClassifier(n_neighbors=temp[0][1], metric='chebyshev')\n",
    "    n.fit(X, y)\n",
    "    y_pred2 = n.predict(X)\n",
    "\n",
    "    return y_pred1, y_pred2\n",
    "\n",
    "# get y_preds\n",
    "y_pred1_3_1, y_pred1_5_1,= knear_cd(under_train1, temp)\n",
    "y_pred2_3_1, y_pred2_5_1 = knear_cd(under_train2, temp)\n",
    "y_pred3_3_1, y_pred3_5_1 = knear_cd(under_train3, temp)\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "# Get confusion matrices\n",
    "cm_list = []\n",
    "cm131 = confusion_matrix(y1_test, y_pred1_3_1)\n",
    "cm_list.append(cm131)\n",
    "\n",
    "cm151 = confusion_matrix(y1_test, y_pred1_5_1)\n",
    "cm_list.append(cm151)\n",
    "\n",
    "cm231 = confusion_matrix(y2_test, y_pred2_3_1)\n",
    "cm_list.append(cm231)\n",
    "\n",
    "cm251 = confusion_matrix(y2_test, y_pred2_5_1)\n",
    "cm_list.append(cm251)\n",
    "\n",
    "cm331 = confusion_matrix(y3_test, y_pred3_3_1)\n",
    "cm_list.append(cm331)\n",
    "\n",
    "cm351 = confusion_matrix(y3_test, y_pred3_5_1)\n",
    "cm_list.append(cm351)\n",
    "\n",
    "\n",
    "# get TSSs\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-journey",
   "metadata": {},
   "source": [
    "---\n",
    "### P9\n",
    "\n",
    "After evaluating the results of P8 things are looking a little less encouraging, since none of those results look to be better than the results of P7. However, the results from P3 weren't really any better than P2 in the first place, so not all is lost.  Let's continue on and see how things turn out with models like we used in P4 since those were actaully an improvement over P2 originally.\n",
    "\n",
    "So, in this question, you will utilize the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), like you did in P4, and try several different settings to see how/if using different settings will improve our score. The difference will again be that you are now using the balanced binary classification datasets constructed in P6 to train 8 different instances for each of your feature selected datasets using the following settings. **(see documentation to know what these are)**\n",
    "\n",
    "|Model Number| criterion | max_depth | splitter |\n",
    "|------------|---------|-------------|---|\n",
    "|1|gini|5|best|\n",
    "|2|gini|5|random|\n",
    "|3|gini|None|best|\n",
    "|4|gini|None|random|\n",
    "|5|entropy|5|best|\n",
    "|6|entropy|5|random|\n",
    "|7|entropy|None|best|\n",
    "|8|entropy|None|random|\n",
    "\n",
    "\n",
    "\n",
    "Once you have done that, test each of your models using your binary classification copy of copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (this should not have been balanced), then calculate and print the TSS score for each result. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "level-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "depth = [5, None]\n",
    "splitter = ['best', 'random']\n",
    "temp = [criterion, depth, splitter]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "caroline-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# get models \n",
    "\n",
    "dt_models1 = dec_tree(under_train1, temp)\n",
    "dt_models2 = dec_tree(under_train2, temp)\n",
    "dt_models3 = dec_tree(under_train3, temp)\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "\n",
    "del cm_list\n",
    "cm_list = []\n",
    "\n",
    "# get confusion_matrices\n",
    "for model in dt_models1:\n",
    "    cm = confusion_matrix(y1_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for model in dt_models2:\n",
    "    cm = confusion_matrix(y2_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for model in dt_models3:\n",
    "    cm = confusion_matrix(y3_test, model)\n",
    "    cm_list.append(cm)\n",
    "\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-bulgaria",
   "metadata": {},
   "source": [
    "---\n",
    "### P10\n",
    "\n",
    "Unlike with [KNeighborsClassifer](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), it seems that the sampling didn't really help much for the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).  Where before we saw a 3X improvement with the Decision Tree over the KNN classifier, we now see similar results for both classifiers.  Let's see how switching to the sampled data affectes our best performing classifier when we were using the full dataset.\n",
    "\n",
    "For this question you will again be utilizing the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier as you did in P5 but using your balanced binary classification dataset constructed in P6 to train just 1 model for each feature selected dataset. Once you have done that, test the model using your binary classification copy of Partition 2 testing dataset that was cunstructed with the same features the model was trained on (this should not have been balanced), then calculate and print the TSS score. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "indirect-albert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSS:  0.0\n",
      "TSS:  0.0\n",
      "TSS:  0.0\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "# get y_preds models\n",
    "gnb_models = []\n",
    "gnb_models.append(gnb(under_train1))\n",
    "gnb_models.append(gnb(under_train2))\n",
    "gnb_models.append(gnb(under_train3))\n",
    "\n",
    "# get y_tests\n",
    "X1_test, y1_test = dichotomize_X_y(test1)\n",
    "X2_test, y2_test = dichotomize_X_y(test2)\n",
    "X3_test, y3_test = dichotomize_X_y(test3)\n",
    "\n",
    "# get confusion matrices\n",
    "del cm_list \n",
    "cm_list = []\n",
    "cm1 = confusion_matrix(y1_test, gnb_models[0])\n",
    "cm_list.append(cm1)\n",
    "cm2 = confusion_matrix(y1_test, gnb_models[1])\n",
    "cm_list.append(cm2)\n",
    "cm3 = confusion_matrix(y1_test, gnb_models[2])\n",
    "cm_list.append(cm3)\n",
    "\n",
    "for cm in cm_list:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tss = tpr - fpr\n",
    "    print('TSS: ', tss)\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbbeda",
   "metadata": {},
   "source": [
    "Unfortunately, we don't see much improvement for our [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) classifier. We Might have to try something different or start over and find more to model on."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
